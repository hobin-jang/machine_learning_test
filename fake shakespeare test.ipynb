{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://homl.info/shakespeare\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "  shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모든 글자 정수로 인코딩\n",
    "keras의 Tokenizer 클래스 사용\n",
    "char_level = True : 단어 수준 대신 글자 수준 인코딩\n",
    "\"\"\"\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20], [6], [9], [8], [3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(\"first\") # 텍스트를 인코딩한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"first\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1115394\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # 고유 글자 개수\n",
    "dataset_size = tokenizer.document_count # 전체 글자 개수\n",
    "print(max_id, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'o': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'h': 7,\n",
       " 's': 8,\n",
       " 'r': 9,\n",
       " 'n': 10,\n",
       " '\\n': 11,\n",
       " 'l': 12,\n",
       " 'd': 13,\n",
       " 'u': 14,\n",
       " 'm': 15,\n",
       " 'y': 16,\n",
       " 'w': 17,\n",
       " ',': 18,\n",
       " 'c': 19,\n",
       " 'f': 20,\n",
       " 'g': 21,\n",
       " 'b': 22,\n",
       " 'p': 23,\n",
       " ':': 24,\n",
       " 'k': 25,\n",
       " 'v': 26,\n",
       " '.': 27,\n",
       " \"'\": 28,\n",
       " ';': 29,\n",
       " '?': 30,\n",
       " '!': 31,\n",
       " '-': 32,\n",
       " 'j': 33,\n",
       " 'q': 34,\n",
       " 'x': 35,\n",
       " 'z': 36,\n",
       " '3': 37,\n",
       " '&': 38,\n",
       " '$': 39}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 # 0부터 인코딩하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 : 전체의 90%, 검증, 테스트 데이터 : 나머지\n",
    "train_size = dataset_size * 90 // 100\n",
    "# 한 번에 한 글자 반환\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "전체 글자 : 백만 개 이상의 시퀀스 하나\n",
    "이를 직접 RNN 훈련 시키면 백만 개의 층이 있는 것과 같다.\n",
    "그러므로 슬라이싱하여 (window 메서드 이용) 텍스트 윈도우로 나누어 부분 문자열을 이용한 RNN을 진행\n",
    "(TBPTT)\n",
    "\"\"\"\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "# window 메서드는 기본적으로 원도우를 중복하지 않음\n",
    "# shift=1 : 한 칸씩 옆으로 움직임 0~100, 1~101, ... default=window_length\n",
    "# drop_remainder = True : 모든 윈도우에 동일한 글자 포함되도록 (여기서는 101개)\n",
    "# 지정하지 않으면 글자 수 점점 줄여나감 101 > 100 > 99 > ... > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "window 메서드는 각각 하나의 데이터셋으로 표현되는 윈도우를 포함하는 데이터셋을 만든다. (중첩 데이터)\n",
    "훈련에는 중첩 데이터셋을 바로 사용할 수 없음 => flat_map 메서드를 이용해 플랫 데이터로 변환\n",
    "{{1,2},{3,4,5,6}}을 flat 시키면 {1,2,3,4,5,6}\n",
    "lambda ds: ds.batch(2) : 각 데이터셋에 적용할 변환 함수를 flat_map 메서드에 전달해야 함\n",
    "위 경우를 전달하면 텐서 2개를 가진 데이터셋으로 변환 \n",
    "{{1,2},{3,4,5,6}} => {[1,2],[3,4],[5,6]}\n",
    "\"\"\"\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "고유 글자 수 적으므로 원-핫 인코딩 사용\n",
    "\"\"\"\n",
    "dataset = dataset.map(lambda x_batch, y_batch: (tf.one_hot(x_batch, depth=max_id), y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들고 훈련 시키기\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                                  dropout=0.2, recurrent_dropout=0.2),\n",
    "              tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "              tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\\nhistory = model.fit(dataset, epochs=20)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=20)\n",
    "\"\"\"\n",
    "# 오래 걸려서 학습 중지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocessing(texts):\n",
    "    x = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(x, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 간단한 다음 글자 예측\n",
    "# 위에서 학습 안 시키면 이상한 글 나옴\n",
    "X_new = preprocessing([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가짜 셰익스피어 텍스트 만들기\n",
    "# 초기 텍스트를 입력하고 모델이 가장 가능성 있는 글자 예측\n",
    "# 이 글자를 텍스트 끝에 추가하고 늘어난 텍스트를 모델에 전달\n",
    "# 이를 반복 (temperature가 0에 가까울 수록 높은 확률의 글자 택함)\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocessing([text])\n",
    "    y_proba = model.predict(X_new)[0,-1:,:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_char=50, temperature=1):\n",
    "    for _ in range(n_char):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgg,h&&zgx!i\n",
      "taplpy,t3m ew h$&?tzak&&ik;uj.lsvbinmf\n",
      "whn!vc njciljdbbi:xkuod,i.tfljl;'uttn;z.k-&sb$rn?ie\n",
      "efcc:j3ypmm\n",
      "fl:bsab&uu,yu'tmpwi&!-,:l!ztq!mly:stwtn\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=0.3))\n",
    "print(complete_text(\"w\", temperature=1))\n",
    "print(complete_text(\"e\", temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위는 학습을 안 시켰기 때문에 이상한 결과 나옴\n",
    "# 16.1.7 이후부터 다시 공부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
